"""
RAG (Retrieval-Augmented Generation) çŸ¥è¯†åº“æœåŠ¡
å®ç°æ–‡æ¡£åŠ è½½ã€é¢„å¤„ç†ã€å‘é‡åŒ–ã€æ£€ç´¢å’Œç”Ÿæˆå¢å¼ºåŠŸèƒ½
"""
from __future__ import annotations

import os
import json
import re
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import numpy as np
from sqlalchemy.orm import Session

from ..models import KnowledgeDocument, KnowledgeChunk
from ..utils import load_env
from .text_cleaner import get_text_cleaner

# å‘é‡æ•°æ®åº“å’ŒåµŒå…¥æ¨¡å‹
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    faiss = None

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    SentenceTransformer = None

# BM25 å…³é”®è¯æ£€ç´¢
try:
    from rank_bm25 import BM25Okapi
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    BM25Okapi = None

# ä¸­æ–‡åˆ†è¯
try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    jieba = None


class RAGService:
    """RAG çŸ¥è¯†åº“æœåŠ¡ç±»"""
    
    def __init__(self):
        load_env()
        self.embedding_model = None
        self.vector_index = None
        self.vector_dim = 384  # é»˜è®¤å‘é‡ç»´åº¦
        self.index_path = Path(__file__).resolve().parent.parent.parent / "knowledge_base_index.faiss"
        self.chunk_size = int(os.environ.get("RAG_CHUNK_SIZE", "500"))  # æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
        self.chunk_overlap = int(os.environ.get("RAG_CHUNK_OVERLAP", "50"))  # å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
        self.top_k = int(os.environ.get("RAG_TOP_K", "5"))  # æ£€ç´¢ Top-K ç›¸å…³æ–‡æ¡£ï¼ˆå¢åŠ åˆ°5ä»¥æé«˜å¬å›ç‡ï¼‰
        self.text_cleaner = get_text_cleaner()
        # BM25ç›¸å…³
        self.bm25_index = None  # BM25ç´¢å¼•
        self.bm25_chunk_texts = []  # å­˜å‚¨æ‰€æœ‰å—çš„æ–‡æœ¬ï¼ˆç”¨äºBM25æ£€ç´¢ï¼‰
        self.bm25_chunk_map = {}  # æ˜ å°„ï¼šBM25ç´¢å¼• -> vector_id
        self.use_hybrid_search = os.environ.get("RAG_USE_HYBRID_SEARCH", "true").lower() == "true"  # æ˜¯å¦ä½¿ç”¨æ··åˆæ£€ç´¢
        self.hybrid_weight_vector = float(os.environ.get("RAG_HYBRID_WEIGHT_VECTOR", "0.7"))  # å‘é‡æ£€ç´¢æƒé‡
        self.hybrid_weight_bm25 = float(os.environ.get("RAG_HYBRID_WEIGHT_BM25", "0.3"))  # BM25æ£€ç´¢æƒé‡
        self._initialize_embedding_model()
        self._load_vector_index()
    
    def _initialize_embedding_model(self, language: Optional[str] = None):
        """
        åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        æ”¯æŒæ ¹æ®è¯­è¨€ç±»å‹é€‰æ‹©ä¸åŒçš„æ¨¡å‹
        
        å‚æ•°:
        - language: è¯­è¨€ç±»å‹ ('zh'/'chinese', 'en'/'english', 'multilingual'/'multi')
        """
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            print("è­¦å‘Š: sentence-transformers æœªå®‰è£…ï¼ŒRAG åŠŸèƒ½å°†å—é™")
            return
        
        # åµŒå…¥æ¨¡å‹æ˜ å°„
        models = {
            'zh': ['BAAI/bge-large-zh-v1.5', 'moka-ai/m3e-large'],
            'chinese': ['BAAI/bge-large-zh-v1.5', 'moka-ai/m3e-large'],
            'multilingual': ['intfloat/multilingual-e5-large', 'text-embedding-3-large'],
            'multi': ['intfloat/multilingual-e5-large', 'text-embedding-3-large'],
            'en': ['text-embedding-ada-002', 'intfloat/e5-large-v2'],
            'english': ['text-embedding-ada-002', 'intfloat/e5-large-v2']
        }
        
        # ç¡®å®šè¯­è¨€ç±»å‹
        if not language:
            language = os.environ.get("RAG_LANGUAGE", "multilingual").lower()
        
        # è·å–æ¨¡å‹åˆ—è¡¨
        model_list = models.get(language, models['multilingual'])
        
        # ä»ç¯å¢ƒå˜é‡æˆ–æ¨¡å‹åˆ—è¡¨ä¸­é€‰æ‹©æ¨¡å‹
        model_name = os.environ.get("RAG_EMBEDDING_MODEL")
        if not model_name:
            # ä½¿ç”¨åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
            model_name = model_list[0]
        
        # å¦‚æœæ¨¡å‹å·²ç»åŠ è½½ï¼Œè·³è¿‡é‡æ–°åŠ è½½
        if self.embedding_model is not None:
            print(f"â„¹ï¸ åµŒå…¥æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡é‡æ–°åŠ è½½: {model_name}")
            return
        
        try:
            print(f"ğŸ“¥ å¼€å§‹åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}...")
            self.embedding_model = SentenceTransformer(model_name)
            # è·å–æ¨¡å‹ç»´åº¦
            test_embedding = self.embedding_model.encode(["test"])
            self.vector_dim = test_embedding.shape[1]
            print(f"âœ“ åµŒå…¥æ¨¡å‹å·²åŠ è½½: {model_name}, ç»´åº¦: {self.vector_dim}, è¯­è¨€: {language}")
        except Exception as e:
            print(f"âš  åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
            if len(model_list) > 1:
                try:
                    model_name = model_list[1]
                    self.embedding_model = SentenceTransformer(model_name)
                    test_embedding = self.embedding_model.encode(["test"])
                    self.vector_dim = test_embedding.shape[1]
                    print(f"âœ“ ä½¿ç”¨å¤‡ç”¨åµŒå…¥æ¨¡å‹: {model_name}, ç»´åº¦: {self.vector_dim}")
                except Exception as e2:
                    print(f"âš  å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                    self.embedding_model = None
            else:
                self.embedding_model = None
    
    def _load_vector_index(self):
        """åŠ è½½æˆ–åˆ›å»ºå‘é‡ç´¢å¼•"""
        if not FAISS_AVAILABLE:
            print("è­¦å‘Š: FAISS æœªå®‰è£…ï¼Œå‘é‡ç´¢å¼•åŠŸèƒ½å°†ä¸å¯ç”¨")
            return
        
        try:
            if self.index_path.exists():
                self.vector_index = faiss.read_index(str(self.index_path))
                print(f"âœ“ å‘é‡ç´¢å¼•å·²åŠ è½½: {self.vector_index.ntotal} ä¸ªå‘é‡")
            else:
                # åˆ›å»ºæ–°çš„ç´¢å¼•
                self.vector_index = faiss.IndexFlatL2(self.vector_dim)
                print("âœ“ åˆ›å»ºæ–°çš„å‘é‡ç´¢å¼•")
        except Exception as e:
            print(f"âš  åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}")
            if self.vector_index is None:
                self.vector_index = faiss.IndexFlatL2(self.vector_dim)
    
    def _save_vector_index(self):
        """ä¿å­˜å‘é‡ç´¢å¼•åˆ°æ–‡ä»¶"""
        if not FAISS_AVAILABLE or self.vector_index is None:
            return
        
        try:
            faiss.write_index(self.vector_index, str(self.index_path))
        except Exception as e:
            print(f"âš  ä¿å­˜å‘é‡ç´¢å¼•å¤±è´¥: {e}")
    
    def _tokenize_chinese(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è¯ï¼ˆç”¨äºBM25ï¼‰"""
        if not text:
            return []
        
        if JIEBA_AVAILABLE:
            # ä½¿ç”¨jiebaåˆ†è¯
            return list(jieba.cut(text))
        else:
            # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼šæŒ‰å­—ç¬¦åˆ†å‰²ï¼ˆå¯¹äºä¸­æ–‡ï¼Œæ¯ä¸ªå­—ç¬¦é€šå¸¸æ˜¯ä¸€ä¸ªè¯ï¼‰
            # åŒæ—¶ä¿ç•™è‹±æ–‡å•è¯
            import re
            # åˆ†ç¦»ä¸­æ–‡å­—ç¬¦å’Œè‹±æ–‡å•è¯
            chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
            english_words = re.findall(r'[a-zA-Z]+', text)
            return chinese_chars + english_words
    
    def _build_bm25_index(self, db: Optional[Session] = None):
        """æ„å»ºBM25ç´¢å¼•ï¼ˆä»æ•°æ®åº“åŠ è½½æ‰€æœ‰å—ï¼‰"""
        if not BM25_AVAILABLE:
            return
        
        if db is None:
            # å»¶è¿Ÿæ„å»ºï¼Œéœ€è¦æ—¶å†æ„å»º
            return
        
        try:
            # ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰æ´»è·ƒçš„å—ï¼ˆä½¿ç”¨æ˜ç¡®çš„joinæ¡ä»¶ï¼‰
            chunks = db.query(KnowledgeChunk).join(
                KnowledgeDocument, KnowledgeChunk.document_id == KnowledgeDocument.id
            ).filter(
                KnowledgeDocument.active == True
            ).order_by(KnowledgeChunk.vector_id.asc()).all()
            
            if not chunks:
                self.bm25_index = None
                self.bm25_chunk_texts = []
                self.bm25_chunk_map = {}
                return
            
            # æ„å»ºBM25ç´¢å¼•
            self.bm25_chunk_texts = []
            self.bm25_chunk_map = {}
            
            for chunk in chunks:
                if chunk.vector_id is not None:
                    # åˆ†è¯å¤„ç†
                    tokenized = self._tokenize_chinese(chunk.content)
                    self.bm25_chunk_texts.append(tokenized)
                    self.bm25_chunk_map[len(self.bm25_chunk_texts) - 1] = chunk.vector_id
            
            if self.bm25_chunk_texts:
                self.bm25_index = BM25Okapi(self.bm25_chunk_texts)
                print(f"âœ“ BM25ç´¢å¼•å·²æ„å»º: {len(self.bm25_chunk_texts)} ä¸ªæ–‡æ¡£å—")
            else:
                self.bm25_index = None
        except Exception as e:
            print(f"âš  æ„å»ºBM25ç´¢å¼•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.bm25_index = None
    
    def search_bm25(self, query: str, top_k: Optional[int] = None, category: Optional[str] = None) -> List[Dict]:
        """
        BM25å…³é”®è¯æ£€ç´¢
        
        å‚æ•°:
        - query: æŸ¥è¯¢æ–‡æœ¬
        - top_k: è¿”å›çš„Top-Kç»“æœæ•°é‡
        - category: åˆ†ç±»ç­›é€‰ï¼ˆå¯é€‰ï¼Œéœ€è¦åœ¨åç»­æ­¥éª¤ä¸­å¤„ç†ï¼‰
        
        è¿”å›:
        - List[Dict]: åŒ…å« vector_id, bm25_score çš„ç»“æœåˆ—è¡¨ï¼ŒæŒ‰BM25åˆ†æ•°é™åºæ’åˆ—
        """
        if not BM25_AVAILABLE or self.bm25_index is None:
            return []
        
        if not query or not query.strip():
            return []
        
        top_k = top_k or self.top_k
        
        try:
            # å¯¹æŸ¥è¯¢æ–‡æœ¬è¿›è¡Œåˆ†è¯
            query_tokens = self._tokenize_chinese(query.strip())
            if not query_tokens:
                return []
            
            # BM25æ£€ç´¢
            scores = self.bm25_index.get_scores(query_tokens)
            
            # æ„å»ºç»“æœåˆ—è¡¨
            results = []
            for idx, score in enumerate(scores):
                vector_id = self.bm25_chunk_map.get(idx)
                if vector_id is not None and score > 0:  # åªä¿ç•™åˆ†æ•°å¤§äº0çš„ç»“æœ
                    results.append({
                        "vector_id": int(vector_id),
                        "bm25_score": float(score)
                    })
            
            # æŒ‰BM25åˆ†æ•°é™åºæ’åº
            results.sort(key=lambda x: x["bm25_score"], reverse=True)
            
            # è¿”å›Top-K
            return results[:top_k * 2]  # è¿”å›æ›´å¤šå€™é€‰ï¼Œç”¨äºæ··åˆæ£€ç´¢
            
        except Exception as e:
            print(f"âš  BM25æ£€ç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def preprocess_text(self, text: str) -> str:
        """
        æ–‡æœ¬é¢„å¤„ç†ï¼šä½¿ç”¨å¢å¼ºçš„æ–‡æœ¬æ¸…æ´—å™¨
        (1) æ–‡æœ¬è§„èŒƒåŒ–
        (2) ç»“æ„åŒ–å¤„ç†
        (3) å†…å®¹æ¸…ç†
        """
        if not text:
            return ""
        
        # ä½¿ç”¨æ–‡æœ¬æ¸…æ´—å™¨è¿›è¡Œè§„èŒƒåŒ–
        normalized = self.text_cleaner.normalize_text(text)
        
        # ç»“æ„åŒ–å¤„ç†
        structured = self.text_cleaner.extract_structure(normalized)
        
        # å†…å®¹æ¸…ç†
        cleaned, quality_info = self.text_cleaner.clean_content(structured["text"])
        
        return cleaned
    
    def chunk_text(self, text: str, chunk_size: Optional[int] = None, overlap: Optional[int] = None) -> List[Dict[str, any]]:
        """
        å°†æ–‡æœ¬åˆ†å—å¤„ç†ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„åˆ†å—ç­–ç•¥ï¼‰
        (4) åˆ†å—ä¼˜åŒ–ï¼šæŒ‰è¯­ä¹‰è¾¹ç•Œåˆ†å—ã€é‡å åˆ†å—ã€æ§åˆ¶å¤§å°
        """
        if not text:
            return []
        
        chunk_size = chunk_size or self.chunk_size
        overlap = overlap or self.chunk_overlap
        
        # ä½¿ç”¨ä¼˜åŒ–çš„åˆ†å—æ–¹æ³•
        chunks = self.text_cleaner.chunk_text_optimized(text, chunk_size, overlap)
        
        return chunks
    
    def embed_text(self, text: str) -> Optional[np.ndarray]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        if not self.embedding_model or not text:
            return None
        
        try:
            embedding = self.embedding_model.encode([text], normalize_embeddings=True)[0]
            return embedding.astype('float32')
        except Exception as e:
            print(f"âš  æ–‡æœ¬å‘é‡åŒ–å¤±è´¥: {e}")
            return None
    
    def embed_texts(self, texts: List[str]) -> Optional[np.ndarray]:
        """
        æ‰¹é‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        æ”¯æŒå­—ç¬¦ä¸²åˆ—è¡¨æˆ–å­—å…¸åˆ—è¡¨ï¼ˆä»å­—å…¸ä¸­æå– contentï¼‰
        """
        if not self.embedding_model or not texts:
            return None
        
        # å¦‚æœæ˜¯å­—å…¸åˆ—è¡¨ï¼Œæå– content å­—æ®µ
        if texts and isinstance(texts[0], dict):
            texts = [item.get("content", "") if isinstance(item, dict) else str(item) for item in texts]
        
        try:
            embeddings = self.embedding_model.encode(texts, normalize_embeddings=True, show_progress_bar=False)
            return embeddings.astype('float32')
        except Exception as e:
            print(f"âš  æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–å¤±è´¥: {e}")
            return None
    
    def add_document(self, db: Session, title: str, content: str, source_type: str = "manual", 
                     source_url: Optional[str] = None, category: Optional[str] = None,
                     tags: Optional[str] = None) -> KnowledgeDocument:
        """
        æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼ˆæ•°æ®å‡†å¤‡é˜¶æ®µï¼‰
        
        å®Œæ•´æµç¨‹ï¼š
        1. æ–‡æœ¬é¢„å¤„ç†ï¼ˆè§„èŒƒåŒ–ã€ç»“æ„åŒ–ã€æ¸…ç†ã€åˆ†å—ã€å…ƒæ•°æ®æå–ï¼‰
        2. å‘é‡åŒ–ï¼ˆä½¿ç”¨åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡ï¼‰
        3. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼ˆFAISSï¼‰
        4. å­˜å‚¨åˆ°å…³ç³»æ•°æ®åº“ï¼ˆSQLiteï¼‰
        
        å‚æ•°:
        - db: æ•°æ®åº“ä¼šè¯
        - title: æ–‡æ¡£æ ‡é¢˜
        - content: æ–‡æ¡£å†…å®¹
        - source_type: æ¥æºç±»å‹ï¼ˆmanual, pdf, web, api, databaseï¼‰
        - source_url: æ¥æºURL
        - category: åˆ†ç±»
        - tags: æ ‡ç­¾
        
        è¿”å›:
        - KnowledgeDocument: åˆ›å»ºçš„æ–‡æ¡£å¯¹è±¡
        """
        # ========== æ­¥éª¤1ï¼šæ–‡æœ¬é¢„å¤„ç†ï¼ˆ5ä¸ªå­æ­¥éª¤ï¼‰==========
        
        # (1) æ–‡æœ¬è§„èŒƒåŒ–
        normalized_content = self.text_cleaner.normalize_text(content)
        if not normalized_content:
            raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ¸…æ´—åä¸ºç©º")
        
        # (2) ç»“æ„åŒ–å¤„ç†
        structured = self.text_cleaner.extract_structure(normalized_content)
        
        # (3) å†…å®¹æ¸…ç†
        cleaned_content, quality_info = self.text_cleaner.clean_content(structured["text"])
        if not cleaned_content:
            raise ValueError(f"æ–‡æ¡£è´¨é‡è¯„åˆ†è¿‡ä½ ({quality_info.get('quality_score', 0):.2f})ï¼Œå·²è¿‡æ»¤")
        
        # (5) å…ƒæ•°æ®æå–
        metadata = self.text_cleaner.extract_metadata(cleaned_content, source_url)
        metadata["quality_score"] = quality_info.get("quality_score", 0.0)
        metadata_json = json.dumps(metadata, ensure_ascii=False)
        
        # (4) åˆ†å—ä¼˜åŒ–
        chunk_data = self.chunk_text(cleaned_content)
        if not chunk_data:
            raise ValueError("æ–‡æ¡£åˆ†å—å¤±è´¥")
        
        # æå–å—å†…å®¹åˆ—è¡¨ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
        chunk_contents = [chunk["content"] for chunk in chunk_data]
        
        # ========== æ­¥éª¤2ï¼šå­˜å‚¨æ–‡æ¡£åˆ°æ•°æ®åº“ ==========
        # åˆ›å»ºæ–‡æ¡£è®°å½•
        doc = KnowledgeDocument(
            title=title,
            content=cleaned_content,  # å­˜å‚¨æ¸…æ´—åçš„å†…å®¹
            source_type=source_type,
            source_url=source_url,
            category=category,
            tags=tags,
            chunk_count=len(chunk_data),
            document_metadata=metadata_json,  # å­˜å‚¨å…ƒæ•°æ®
            quality_score=metadata["quality_score"]  # å­˜å‚¨è´¨é‡è¯„åˆ†
        )
        db.add(doc)
        db.flush()
        
        # ========== æ­¥éª¤3ï¼šå‘é‡åŒ–ä¸ç´¢å¼•æ„å»º ==========
        SYNTHETIC_OFFSET = 1000000  # å‘é‡åŒ–å¤±è´¥æ—¶ä½¿ç”¨çš„åˆæˆ ID å‰ç¼€ï¼Œé¿å…ä¸ FAISS å†²çª
        chunks_created = False
        
        if self.embedding_model and FAISS_AVAILABLE and self.vector_index is not None:
            # 3.1 æ‰¹é‡å‘é‡åŒ–ï¼ˆä½¿ç”¨åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡ï¼‰
            embeddings = self.embed_texts(chunk_contents)
            if embeddings is not None:
                start_id = self.vector_index.ntotal
                
                # 3.2 æ·»åŠ åˆ°FAISSå‘é‡ç´¢å¼•ï¼ˆç”¨äºå¿«é€Ÿç›¸ä¼¼åº¦æ£€ç´¢ï¼‰
                self.vector_index.add(embeddings)
                
                # 3.3 åˆ›å»ºå—è®°å½•å¹¶å­˜å‚¨åˆ°æ•°æ®åº“ï¼ˆåŒ…å«å‘é‡IDæ˜ å°„ï¼‰
                for i, (chunk_info, embedding) in enumerate(zip(chunk_data, embeddings)):
                    # å—å…ƒæ•°æ®
                    chunk_metadata = {
                        "title": chunk_info.get("title"),
                        "type": chunk_info.get("type", "paragraph"),
                        "chunk_index": chunk_info.get("chunk_index", i)
                    }
                    
                    chunk_record = KnowledgeChunk(
                        document_id=doc.id,
                        chunk_index=i,
                        content=chunk_info["content"],
                        chunk_metadata=json.dumps(chunk_metadata, ensure_ascii=False),
                        vector_id=start_id + i  # å­˜å‚¨å‘é‡IDï¼Œç”¨äºæ£€ç´¢æ—¶æ˜ å°„
                    )
                    db.add(chunk_record)
                    chunks_created = True
                
                print(f"  â†’ å·²å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°FAISSç´¢å¼•ï¼ˆå‘é‡ID: {start_id}-{start_id + len(chunk_data) - 1}ï¼‰")
            else:
                print(f"  â†’ å‘é‡åŒ–å¤±è´¥ï¼Œå°†ä»…åˆ›å»º chunks ä¾› BM25 æ£€ç´¢")
        
        # å‘é‡åŒ–å¤±è´¥æˆ–æ— åµŒå…¥æ¨¡å‹æ—¶ï¼šä»åˆ›å»º chunksï¼Œä½¿ç”¨åˆæˆ vector_id ä¾› BM25 æ£€ç´¢
        if not chunks_created:
            for i, chunk_info in enumerate(chunk_data):
                chunk_metadata = {
                    "title": chunk_info.get("title"),
                    "type": chunk_info.get("type", "paragraph"),
                    "chunk_index": chunk_info.get("chunk_index", i)
                }
                synthetic_id = SYNTHETIC_OFFSET + doc.id * 10000 + i
                chunk_record = KnowledgeChunk(
                    document_id=doc.id,
                    chunk_index=i,
                    content=chunk_info["content"],
                    chunk_metadata=json.dumps(chunk_metadata, ensure_ascii=False),
                    vector_id=synthetic_id  # åˆæˆ IDï¼Œä»…ç”¨äº BM25 æ£€ç´¢
                )
                db.add(chunk_record)
        
        # 3.4 æäº¤æ•°æ®åº“äº‹åŠ¡
        db.commit()
        db.refresh(doc)
        
        # 3.5 ä¿å­˜å‘é‡ç´¢å¼•åˆ°æ–‡ä»¶ï¼ˆæŒä¹…åŒ–ï¼‰
        self._save_vector_index()
        
        # 3.6 é‡å»ºBM25ç´¢å¼•ï¼ˆå¦‚æœå¯ç”¨æ··åˆæ£€ç´¢ï¼‰
        if self.use_hybrid_search and BM25_AVAILABLE:
            self._build_bm25_index(db)
        
        print(f"âœ“ æ–‡æ¡£å·²æ·»åŠ : {title}, å—æ•°: {len(chunk_data)}, è´¨é‡è¯„åˆ†: {metadata['quality_score']:.2f}")
        
        return doc
    
    def search(self, query: str, top_k: Optional[int] = None, category: Optional[str] = None, 
               similarity_threshold: float = 0.15) -> List[Dict]:  # é™ä½é»˜è®¤é˜ˆå€¼ä»¥æé«˜å¬å›ç‡
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£å—ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        å‚æ•°:
        - query: æŸ¥è¯¢æ–‡æœ¬
        - top_k: è¿”å›çš„Top-Kç»“æœæ•°é‡
        - category: åˆ†ç±»ç­›é€‰ï¼ˆå¯é€‰ï¼‰
        - similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œä½äºæ­¤å€¼çš„ç»“æœä¼šè¢«è¿‡æ»¤
        
        è¿”å›:
        - List[Dict]: åŒ…å« vector_id, distance, similarity çš„ç»“æœåˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        if not self.embedding_model or not FAISS_AVAILABLE or self.vector_index is None:
            return []
        
        if not query or not query.strip():
            return []
        
        top_k = top_k or self.top_k
        
        # å‘é‡åŒ–æŸ¥è¯¢ï¼ˆä½¿ç”¨ä¸æ–‡æ¡£ç›¸åŒçš„åµŒå…¥æ¨¡å‹ï¼‰
        query_embedding = self.embed_text(query.strip())
        if query_embedding is None:
            return []
        
        # å‘é‡ç´¢å¼•ä¸ºç©ºæ—¶ç›´æ¥è¿”å›ï¼Œé¿å… FAISS çš„ assert k > 0 æŠ¥é”™
        if self.vector_index.ntotal == 0:
            return []
        
        # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…ï¼ˆæ—§ç´¢å¼•å¯èƒ½ç”±ä¸åŒåµŒå…¥æ¨¡å‹åˆ›å»ºï¼‰
        if self.vector_index.d != query_embedding.shape[0]:
            print(f"âš  å‘é‡ç»´åº¦ä¸åŒ¹é…ï¼šç´¢å¼• {self.vector_index.d} vs æŸ¥è¯¢ {query_embedding.shape[0]}ï¼Œè·³è¿‡å‘é‡æ£€ç´¢")
            return []
        
        # æœç´¢å‘é‡ç´¢å¼•ï¼ˆä½¿ç”¨L2è·ç¦»ï¼Œåç»­è½¬æ¢ä¸ºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        query_embedding = query_embedding.reshape(1, -1)
        max_results = min(top_k * 3, self.vector_index.ntotal)  # å¤šæ£€ç´¢ä¸€äº›ï¼Œç”¨äºåç»­ç­›é€‰ï¼ˆä»2å€å¢åŠ åˆ°3å€ä»¥æé«˜å¬å›ç‡ï¼‰
        distances, indices = self.vector_index.search(query_embedding, max_results)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆFAISSä½¿ç”¨L2è·ç¦»ï¼Œéœ€è¦è½¬æ¢ä¸ºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        # å¯¹äºå½’ä¸€åŒ–çš„å‘é‡ï¼ŒL2è·ç¦»å’Œä½™å¼¦è·ç¦»çš„å…³ç³»ï¼šcosine_sim = 1 - (L2_distance^2 / 2)
        # æˆ–è€…ä½¿ç”¨ï¼šsimilarity = 1 / (1 + distance)
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx < 0:  # FAISS è¿”å› -1 è¡¨ç¤ºæ— æ•ˆ
                continue
            
            # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼Œ1è¡¨ç¤ºæœ€ç›¸ä¼¼ï¼‰
            # å¯¹äºL2è·ç¦»ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„è½¬æ¢ï¼šå¯¹äºå½’ä¸€åŒ–å‘é‡ï¼Œcosine_sim â‰ˆ 1 - distance^2/2
            # ä½†è€ƒè™‘åˆ°å®é™…ä½¿ç”¨ï¼Œä½¿ç”¨æ›´å®½æ¾çš„è½¬æ¢ä»¥æé«˜å¬å›ç‡
            if dist <= 0:
                similarity = 1.0
            else:
                # ä½¿ç”¨æ›´å®½æ¾çš„è½¬æ¢å…¬å¼ï¼Œé™ä½é˜ˆå€¼è¦æ±‚
                similarity = float(1 / (1 + dist * 0.5))  # è°ƒæ•´ç³»æ•°ä»¥æé«˜ç›¸ä¼¼åº¦åˆ†æ•°
            
            # åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
            if similarity < similarity_threshold:
                continue
            
            results.append({
                "vector_id": int(idx),
                "distance": float(dist),
                "similarity": similarity
            })
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åºï¼ˆæœ€ç›¸ä¼¼çš„åœ¨å‰é¢ï¼‰
        results.sort(key=lambda x: x["similarity"], reverse=True)
        
        # è¿”å›Top-K
        return results[:top_k]
    
    def retrieve_context(self, db: Session, query: str, top_k: Optional[int] = None, 
                        category: Optional[str] = None, similarity_threshold: float = 0.3) -> Tuple[str, List[Dict]]:
        """
        æ£€ç´¢å¹¶è¿”å›ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆRAGæ£€ç´¢å¢å¼ºçš„æ ¸å¿ƒæ–¹æ³•ï¼‰
        
        å‚æ•°:
        - db: æ•°æ®åº“ä¼šè¯
        - query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
        - top_k: è¿”å›çš„Top-Kç›¸å…³å—
        - category: åˆ†ç±»ç­›é€‰ï¼ˆå¯é€‰ï¼‰
        - similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç»“æœä¼šè¢«è¿‡æ»¤
        
        è¿”å›:
        - Tuple[str, List[Dict]]: (ä¸Šä¸‹æ–‡æ–‡æœ¬, æ£€ç´¢ç»“æœè¯¦æƒ…)
        """
        # æ­¥éª¤1ï¼šæ··åˆæ£€ç´¢ï¼ˆå‘é‡æ£€ç´¢ + BM25å…³é”®è¯æ£€ç´¢ï¼‰
        # ç¡®ä¿BM25ç´¢å¼•å·²æ„å»ºï¼ˆå¦‚æœå¯ç”¨æ··åˆæ£€ç´¢ï¼‰
        if self.use_hybrid_search and BM25_AVAILABLE and self.bm25_index is None:
            self._build_bm25_index(db)
        
        # 1.1 å‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
        initial_threshold = max(0.1, similarity_threshold * 0.6)  # ä½¿ç”¨æ›´ä½çš„åˆå§‹é˜ˆå€¼
        expanded_top_k = top_k * 3  # æ£€ç´¢æ›´å¤šå€™é€‰ç»“æœ
        
        vector_results = self.search(query, expanded_top_k, category, initial_threshold)
        
        if not vector_results:
            # å¦‚æœåˆå§‹æ£€ç´¢æ²¡æœ‰ç»“æœï¼Œå°è¯•è¿›ä¸€æ­¥é™ä½é˜ˆå€¼
            vector_results = self.search(query, expanded_top_k, category, 0.05)
        
        # 1.2 BM25å…³é”®è¯æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨æ··åˆæ£€ç´¢ï¼‰
        bm25_results = []
        if self.use_hybrid_search and BM25_AVAILABLE:
            # ç¡®ä¿BM25ç´¢å¼•å·²æ„å»º
            if self.bm25_index is None:
                self._build_bm25_index(db)
            
            if self.bm25_index is not None:
                bm25_results = self.search_bm25(query, expanded_top_k, category)
                print(f"âœ“ BM25æ£€ç´¢: æ‰¾åˆ° {len(bm25_results)} ä¸ªå€™é€‰ç»“æœ")
        
        # 1.3 åˆå¹¶å’Œé‡æ’åºç»“æœ
        if not vector_results and not bm25_results:
            return "", []
        
        # å½’ä¸€åŒ–åˆ†æ•°å¹¶åˆå¹¶ç»“æœ
        combined_results = {}
        # ä¿å­˜åŸå§‹å‘é‡æ£€ç´¢ç»“æœï¼Œç”¨äºåç»­è·å–distanceå­—æ®µ
        vector_results_map = {r["vector_id"]: r for r in vector_results}
        
        # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
        if vector_results:
            # å½’ä¸€åŒ–ç›¸ä¼¼åº¦åˆ†æ•°åˆ°0-1èŒƒå›´
            max_sim = max([r["similarity"] for r in vector_results], default=1.0)
            min_sim = min([r["similarity"] for r in vector_results], default=0.0)
            sim_range = max_sim - min_sim if max_sim > min_sim else 1.0
            
            for r in vector_results:
                vector_id = r["vector_id"]
                # å½’ä¸€åŒ–ç›¸ä¼¼åº¦
                normalized_sim = (r["similarity"] - min_sim) / sim_range if sim_range > 0 else r["similarity"]
                combined_results[vector_id] = {
                    "vector_id": vector_id,
                    "vector_score": normalized_sim * self.hybrid_weight_vector,
                    "bm25_score": 0.0,
                    "combined_score": normalized_sim * self.hybrid_weight_vector,
                    "distance": r.get("distance", 0.0)  # ä¿ç•™åŸå§‹distanceå­—æ®µ
                }
        
        # å¤„ç†BM25æ£€ç´¢ç»“æœ
        if bm25_results:
            # å½’ä¸€åŒ–BM25åˆ†æ•°åˆ°0-1èŒƒå›´
            max_bm25 = max([r["bm25_score"] for r in bm25_results], default=1.0)
            min_bm25 = min([r["bm25_score"] for r in bm25_results], default=0.0)
            bm25_range = max_bm25 - min_bm25 if max_bm25 > min_bm25 else 1.0
            
            for r in bm25_results:
                vector_id = r["vector_id"]
                # å½’ä¸€åŒ–BM25åˆ†æ•°
                normalized_bm25 = (r["bm25_score"] - min_bm25) / bm25_range if bm25_range > 0 else r["bm25_score"]
                
                if vector_id in combined_results:
                    # åˆå¹¶åˆ†æ•°
                    combined_results[vector_id]["bm25_score"] = normalized_bm25 * self.hybrid_weight_bm25
                    combined_results[vector_id]["combined_score"] += normalized_bm25 * self.hybrid_weight_bm25
                else:
                    # æ–°ç»“æœï¼ˆä»BM25æ£€ç´¢ä¸­è·å–ï¼Œå¯èƒ½æ²¡æœ‰distanceå­—æ®µï¼‰
                    original_vector_result = vector_results_map.get(vector_id, {})
                    combined_results[vector_id] = {
                        "vector_id": vector_id,
                        "vector_score": 0.0,
                        "bm25_score": normalized_bm25 * self.hybrid_weight_bm25,
                        "combined_score": normalized_bm25 * self.hybrid_weight_bm25,
                        "distance": original_vector_result.get("distance", 0.0)  # å¦‚æœå‘é‡æ£€ç´¢ä¸­æœ‰ï¼Œåˆ™ä½¿ç”¨
                    }
        
        # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
        search_results = sorted(combined_results.values(), key=lambda x: x["combined_score"], reverse=True)
        
        # å–top_kä¸ªç»“æœ
        search_results = search_results[:top_k]
        
        # è½¬æ¢ä¸ºåŸæœ‰æ ¼å¼ï¼ˆå…¼å®¹æ€§ï¼‰
        final_results = []
        for r in search_results:
            final_results.append({
                "vector_id": r["vector_id"],
                "similarity": r["combined_score"],  # ä½¿ç”¨ç»¼åˆåˆ†æ•°ä½œä¸ºç›¸ä¼¼åº¦
                "vector_score": r["vector_score"],
                "bm25_score": r["bm25_score"],
                "distance": r.get("distance", 0.0)  # ä¿ç•™distanceå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            })
        
        search_results = final_results
        
        if search_results:
            print(f"âœ“ æ··åˆæ£€ç´¢å®Œæˆ: å‘é‡æ£€ç´¢ {len(vector_results)} ä¸ªï¼ŒBM25æ£€ç´¢ {len(bm25_results)} ä¸ªï¼Œåˆå¹¶å {len(search_results)} ä¸ª")
        
        # æ­¥éª¤2ï¼šä»æ•°æ®åº“è·å–å—å†…å®¹
        vector_ids = [r["vector_id"] for r in search_results]
        chunks = db.query(KnowledgeChunk).filter(
            KnowledgeChunk.vector_id.in_(vector_ids),
            KnowledgeChunk.document_id.in_(
                db.query(KnowledgeDocument.id).filter(KnowledgeDocument.active == True)
            )
        ).all()
        
        # å¦‚æœæŒ‡å®šäº†åˆ†ç±»ï¼Œè¿›ä¸€æ­¥ç­›é€‰
        if category:
            doc_ids = [d.id for d in db.query(KnowledgeDocument.id).filter(
                KnowledgeDocument.category == category,
                KnowledgeDocument.active == True
            ).all()]
            chunks = [c for c in chunks if c.document_id in doc_ids]
        
        # æ­¥éª¤3ï¼šæŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶ç»„åˆä¸Šä¸‹æ–‡
        chunk_dict = {c.vector_id: c for c in chunks}
        context_parts = []
        result_details = []
        
        for result in search_results:
            chunk = chunk_dict.get(result["vector_id"])
            if chunk:
                # è·å–æ–‡æ¡£ä¿¡æ¯
                doc = db.query(KnowledgeDocument).filter(KnowledgeDocument.id == chunk.document_id).first()
                doc_title = doc.title if doc else f"æ–‡æ¡£#{chunk.document_id}"
                
                # æ„å»ºä¸Šä¸‹æ–‡ç‰‡æ®µï¼ˆç§»é™¤ç›¸ä¼¼åº¦æ ‡æ³¨ï¼Œé¿å…å¹²æ‰°AIåˆ¤æ–­ï¼‰
                context_parts.append(chunk.content)
                
                # ä¿å­˜ç»“æœè¯¦æƒ…
                result_details.append({
                    "chunk_id": chunk.id,
                    "document_id": chunk.document_id,
                    "document_title": doc_title,
                    "chunk_index": chunk.chunk_index,
                    "content": chunk.content,
                    "similarity": result["similarity"],
                    "distance": result.get("distance", 0.0),  # æ··åˆæ£€ç´¢å¯èƒ½æ²¡æœ‰distanceå­—æ®µ
                    "vector_score": result.get("vector_score", 0.0),
                    "bm25_score": result.get("bm25_score", 0.0)
                })
        
        context_text = "\n\n".join(context_parts)
        return context_text, result_details
    
    def delete_document(self, db: Session, document_id: int):
        """åˆ é™¤æ–‡æ¡£ï¼ˆéœ€è¦é‡å»ºç´¢å¼•ï¼‰"""
        doc = db.query(KnowledgeDocument).filter(KnowledgeDocument.id == document_id).first()
        if not doc:
            return
        
        # è·å–æ‰€æœ‰å—çš„å‘é‡ ID
        chunks = db.query(KnowledgeChunk).filter(KnowledgeChunk.document_id == document_id).all()
        vector_ids = [c.vector_id for c in chunks if c.vector_id is not None]
        
        # åˆ é™¤å—è®°å½•
        db.query(KnowledgeChunk).filter(KnowledgeChunk.document_id == document_id).delete()
        
        # åˆ é™¤æ–‡æ¡£
        db.delete(doc)
        db.commit()
        
        # é‡å»ºç´¢å¼•ï¼ˆåˆ é™¤æ“ä½œè¾ƒå¤æ‚ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼šé‡å»ºæ•´ä¸ªç´¢å¼•ï¼‰
        self._rebuild_index(db)
        
        # é‡å»ºBM25ç´¢å¼•
        if self.use_hybrid_search and BM25_AVAILABLE:
            self._build_bm25_index(db)
    
    def _rebuild_index(self, db: Session):
        """é‡å»ºå‘é‡ç´¢å¼•"""
        if not FAISS_AVAILABLE or not self.embedding_model:
            return
        
        # è·å–æ‰€æœ‰æ´»è·ƒæ–‡æ¡£çš„å—ï¼ˆä½¿ç”¨æ˜ç¡®çš„joinæ¡ä»¶ï¼‰
        chunks = db.query(KnowledgeChunk).join(
            KnowledgeDocument, KnowledgeChunk.document_id == KnowledgeDocument.id
        ).filter(
            KnowledgeDocument.active == True
        ).order_by(KnowledgeChunk.document_id, KnowledgeChunk.chunk_index).all()
        
        if not chunks:
            self.vector_index = faiss.IndexFlatL2(self.vector_dim)
            self._save_vector_index()
            return
        
        # é‡æ–°å‘é‡åŒ–ï¼ˆchunks æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
        texts = [c.content for c in chunks]
        embeddings = self.embed_texts(texts)
        
        if embeddings is not None:
            # åˆ›å»ºæ–°ç´¢å¼•
            self.vector_index = faiss.IndexFlatL2(self.vector_dim)
            self.vector_index.add(embeddings)
            
            # æ›´æ–°å‘é‡ ID
            for i, chunk in enumerate(chunks):
                chunk.vector_id = i
            db.commit()
            
            self._save_vector_index()
            print(f"âœ“ å‘é‡ç´¢å¼•å·²é‡å»º: {len(chunks)} ä¸ªå—")
            
            # é‡å»ºBM25ç´¢å¼•
            if self.use_hybrid_search and BM25_AVAILABLE:
                self._build_bm25_index(db)
    
    def rebuild_chunks_for_documents_without_chunks(self, db: Session) -> int:
        """
        ä¸ºæ²¡æœ‰ chunks çš„æ–‡æ¡£é‡å»º chunksï¼ˆå‘é‡åŒ–å¤±è´¥æ—¶ä»åˆ›å»º chunks ä¾› BM25 æ£€ç´¢ï¼‰
        è¿”å›å¤„ç†çš„æ–‡æ¡£æ•°é‡
        """
        SYNTHETIC_OFFSET = 1000000
        docs = db.query(KnowledgeDocument).filter(
            KnowledgeDocument.active == True,
            KnowledgeDocument.content.isnot(None),
            KnowledgeDocument.content != ""
        ).all()
        count = 0
        for doc in docs:
            chunk_count = db.query(KnowledgeChunk).filter(KnowledgeChunk.document_id == doc.id).count()
            if chunk_count > 0:
                continue
            try:
                chunk_data = self.chunk_text(doc.content)
                if not chunk_data:
                    continue
                for i, chunk_info in enumerate(chunk_data):
                    synthetic_id = SYNTHETIC_OFFSET + doc.id * 10000 + i
                    chunk_record = KnowledgeChunk(
                        document_id=doc.id,
                        chunk_index=i,
                        content=chunk_info["content"],
                        chunk_metadata=json.dumps({
                            "title": chunk_info.get("title"),
                            "type": chunk_info.get("type", "paragraph"),
                            "chunk_index": chunk_info.get("chunk_index", i)
                        }, ensure_ascii=False),
                        vector_id=synthetic_id
                    )
                    db.add(chunk_record)
                doc.chunk_count = len(chunk_data)
                count += 1
                print(f"  â†’ å·²ä¸ºæ–‡æ¡£ #{doc.id}ã€Š{doc.title}ã€‹é‡å»º {len(chunk_data)} ä¸ª chunks")
            except Exception as e:
                print(f"âš  æ–‡æ¡£ #{doc.id} é‡å»º chunks å¤±è´¥: {e}")
        if count > 0:
            db.commit()
            if self.use_hybrid_search and BM25_AVAILABLE:
                self._build_bm25_index(db)
            print(f"âœ“ å·²ä¸º {count} ä¸ªæ–‡æ¡£é‡å»º chunks")
        return count


# å…¨å±€ RAG æœåŠ¡å®ä¾‹
_rag_service: Optional[RAGService] = None


def is_rag_ready() -> bool:
    """æ£€æŸ¥ RAG æœåŠ¡æ˜¯å¦å·²å‡†å¤‡å¥½ï¼ˆæ¨¡å‹å·²åŠ è½½ï¼‰"""
    global _rag_service
    if _rag_service is None:
        return False
    return _rag_service.embedding_model is not None


def get_rag_service() -> RAGService:
    """è·å– RAG æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _rag_service
    if _rag_service is None:
        print("ğŸ”„ åˆ›å»ºæ–°çš„ RAG æœåŠ¡å®ä¾‹ï¼ˆé¦–æ¬¡åˆå§‹åŒ–ï¼‰")
        _rag_service = RAGService()
    else:
        print("â™»ï¸ ä½¿ç”¨ç°æœ‰çš„ RAG æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰")
    return _rag_service
